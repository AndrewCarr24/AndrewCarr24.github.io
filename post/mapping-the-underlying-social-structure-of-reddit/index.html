<!DOCTYPE html>
<html lang="en-us">
    <head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>
				Mapping the Underlying Social Structure of Reddit &middot; Data Science Diarist
		</title>


  		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">


		<link href="" rel="alternate" type="application/rss+xml" title="Data Science Diarist" />
	</head>

    <body>
        <nav class="nav">
  <div class="nav-container">
    <a href="/tale/">
      <h2 class="nav-title">Data Diarist</h2>
    </a>
    <ul>
      <li><a href="/tale/about">About</a></li>
      <li><a href="/tale/">Github</a></li>
    </ul>
  </div>
</nav>

 <link rel="stylesheet"
href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/github.min.css">
 <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js">
</script> <script>hljs.initHighlightingOnLoad();</script>



<main>
	<div class="post">
		<div class="post-info">
    <span>Written by</span>
        Andrew Carr
        <br>
        <span>on&nbsp;</span><time datetime="2019-07-22 00:00:00 &#43;0000 UTC">July 22, 2019</time>
</div>
		<h1 class="post-title">Mapping the Underlying Social Structure of Reddit</h1>
<div class="post-line"></div>




<script src="/post/mapping-the-underlying-social-structure-of-reddit/htmlwidgets.js"></script>
<link href="/post/mapping-the-underlying-social-structure-of-reddit/vis.css" rel="stylesheet" />
<script src="/post/mapping-the-underlying-social-structure-of-reddit/vis.min.js"></script>
<script src="/post/mapping-the-underlying-social-structure-of-reddit/visNetwork.js"></script>


<p>Since it was founded in 2005, Reddit has developed into a popular place for sharing opinions and ideas, rating web content, and aggregating news on the internet. Reddit is organized into thousands of user-made communities, called subreddits, which cover a broad range of subjects, including politics, sports, technology, personal hobbies, and self-improvement, to name a few. Given that Reddit is structured in this way, it is natural to think of Reddit as a population of users organized into many overlapping communities (subreddits). In other words, it makes sense to conceptualize Reddit as having an underlying social structure. Uncovering this structure may provide insights into the social organization of internet culture more generally.</p>
<p>My goal in this post is to map the social structure of Reddit by measuring the proximity of Reddit communities to each other. I’m operationalizing proximity between subreddits as the number of submissions on these subreddits that come from the same user in a given time period. For example, if a user posts something to subreddit A and then a few days later posts something else to subreddit B, subreddits A and B are linked by this user. Subreddits with more users in common are closer together. The idea that networks between groups are formed by the people these groups have in common is an old one in sociology (<a href="https://www.jstor.org/stable/2576011?seq=1#metadata_info_tab_contents">Breiger 1974</a>); more recently, it has served as a conceptual basis for producing networks from internet data (<a href="https://www-cs.stanford.edu/~jure/pubs/cesna-icdm13.pdf">Yang et al. 2013</a>; <a href="https://cs.stanford.edu/people/jure/pubs/agmfit-icdm12.pdf">Yang and Leskovec 2012</a>). To my knowledge, this idea hasn’t been used to examine the social structure of communities on Reddit, although other methods have been used to analyze the community structure of Reddit (<a href="https://peerj.com/articles/cs-4/">Olson and Neal 2015</a>).</p>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The data I’m going to use for this post come from a marvelous online repository of subreddit submissions, comments, and other content that is generously hosted by data scientist Jason Baumgartner. Although I’m only making use of the submissions section of Baumgartner’s repository, I’ve downloaded a lot of his data. If you plan to download a lot of data from this repo, I implore you to donate a bit of money to keep Baumgartner’s database up and running (donate here - <a href="https://pushshift.io/donations/">pushshift.io/donations/</a>). Hosting this data is not free!</p>
<p>Here’s the link to Baumgarter’s Reddit submissions data - <a href="http://files.pushshift.io/reddit/submissions/">files.pushshift.io/reddit/submissions/</a>. Each of these files has all Reddit submissions for a given month between June 2005 and May 2019. Files are JSON objects stored in various compression formats that range between .017Mb and 5.77Gb in size, a testament to how much Reddit has grown over the years. Let’s get our feet wet with this data by downloading something in the middle of this range - a 710Mb file for all Reddit submissions in May 2013. The file is called RS_2013-05.bz2. It will take a minute or to download. You can double-click this file to unzip it, or if you prefer to work in the Terminal use the following command: <code>bzip2 -d RS_2013-05.bz2</code>. This will take a couple of minutes to unzip. Make sure you have enough room to store the unzipped file on your computer - it’s 4.51Gb.<br />
Once we have unzipped this file, load the readr, jsonlite, and dplyr packages. Use the read_lines function from readr and set the n_max parameter to 1 to read the first line from the file. Pipe this through fromJSON and names to get a list of the variable names contained in the data.</p>
<pre class="r"><code>read_lines(&quot;RS_2013-05&quot;, n_max = 1) %&gt;% fromJSON() %&gt;% names</code></pre>
<pre><code>##  [1] &quot;edited&quot;                 &quot;title&quot;
##  [3] &quot;thumbnail&quot;              &quot;retrieved_on&quot;
##  [5] &quot;mod_reports&quot;            &quot;selftext_html&quot;
##  [7] &quot;link_flair_css_class&quot;   &quot;downs&quot;
##  [9] &quot;over_18&quot;                &quot;secure_media&quot;
## [11] &quot;url&quot;                    &quot;author_flair_css_class&quot;
## [13] &quot;media&quot;                  &quot;subreddit&quot;
## [15] &quot;author&quot;                 &quot;user_reports&quot;
## [17] &quot;domain&quot;                 &quot;created_utc&quot;
## [19] &quot;stickied&quot;               &quot;secure_media_embed&quot;
## [21] &quot;media_embed&quot;            &quot;ups&quot;
## [23] &quot;distinguished&quot;          &quot;selftext&quot;
## [25] &quot;num_comments&quot;           &quot;banned_by&quot;
## [27] &quot;score&quot;                  &quot;report_reasons&quot;
## [29] &quot;id&quot;                     &quot;gilded&quot;
## [31] &quot;is_self&quot;                &quot;subreddit_id&quot;
## [33] &quot;link_flair_text&quot;        &quot;permalink&quot;
## [35] &quot;author_flair_text&quot;</code></pre>
<p>The data contain 35 bits of information for each submission. For this project, I’m only interested in three things: the user name associated with the submission (author), the subreddit to which a submission has been posted (subreddit), and the time of submission (created_utc). Everything else is extraneous information.</p>
<p>If we could figure out a way to extract these three pieces of information from each line of JSON we could greatly reduce the size of our data, which would allow us to store multiple months worth of information on our local machine. Jq is a command-line JSON processor that makes this possible.</p>
<p>I’m going to walk you through installing jq on a Mac (sorry Windows users!). First, you need to make sure you have Home Brew installed (<a href="https://brew.sh/">brew.sh/</a>). Home Brew is a package manager for the Mac that works in the Terminal. The following instructions assume you have Home Brew installed. To begin, open the Terminal (press Cmd+Space, type Terminal, press Enter). To install jq, type <code>brew install jq</code>. Next, we’ll extract the variables we want from RS_2015-03 and save the result as a .csv file, all in one file of code. To select variables with jq, we want to list the JSON field names that we want like this: <code>[.author, .created_utc, .subreddit]</code>. We want to return these as raw output (<code>-r</code>) and we want this outtput to be rendered as a csv file (<code>@csv</code>). Taken together, the magic command that accomplishes what we want is this:</p>
<p><code>jq -r '[.author, .created_utc, .subreddit] | @csv' RS_2013-05  &gt; parsed_json_to_csv_2013_05</code></p>
<p>Make sure the Terminal directory is set to wherever RS_2013-05 is located before running this command. I am by no means a master of jq syntax, but what I think this command says is to pull “author”, “created_utc”, and “subreddit” from each line of the JSON file and render the output as a .csv file. The pipe operator (“|”) takes output to its right and applies the command to its left to this output. The resultant file will be saved as “parsed_json_to_csv_2013_05”. The jq command is pulling these fields from millions of lines of JSON (every Reddit submission from 03-2015), so this process can take a few minutes. In case you’re new to working in the Terminal, if there’s a blank line at the bottom of the Terminal window, that means the process is still running. When the directory name followed by a dollar sign reappears, the process is complete. The file parsed_json_to_csv_2013_05 should now appear in whatever directory you’re in in the Terminal. This file is about 118Mb, much smaller than 4.5Gb.</p>
<p>Jq can be a powerful tool for automating the process of downloading and manipulating data right from your harddrive. I’ve written the a bash script that allows you to download multiple files from the Reddit repository, unzip them, extract the relevant fields from the resulting JSON, and delete the unparsed files (<a href="https://github.com/datadiarist/Reddit-Analysis-Files/blob/master/Reddit_Download_Script.bash">Reddit_Download_Script.bash</a>). You can modify this script to pull different fields from the JSON. For instance, if you want to keep the content of Reddit submissions, add <code>.selftext</code> to the fields that included in the brackets. Again, if you plan to download a lot of data, please donate to the repository.</p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<p>We are now ready to transform this data into a form that will allow us to analyze Reddit as a network. In its current form, each row of our data represents a user submission. What we want is a dataframe where each row represents a link between subreddits through a common user. This link exists because the user has submitted posts to both subreddits.</p>
<p>One problem that may arise from this kind of data manipulation is that there are likely many more rows in the network form of this data than there are in the original form of the data. To see this, consider a user who has submitted posts to 10 distinct subreddits. These submissions would take up ten rows of our data in its current form. How many rows would we need to represent this in the network form of the data? To answer this question, we need to figure out how many pairs of subreddits are connected through this user. This is the same as finding how many distinct pairs of subreddits can be taken from the 10 subreddits to which the user has submitted. In combinatorics terms, this equals 10 choose 2, or <span class="math inline">\(\frac{10!}{(10-2)!2!} = 45\)</span> &lt;– fix this. This number gets exponentially larger as the number of submissions from the same user increases. In sum, although our data is only ~118Mb, this data will get a lot bigger once we convert it into a network format.</p>
<p>For this reason, I chose to import this data as a Spark dataframe. Spark is a distributed computing platform that partitions large datasets and allows you operate on these partitions in parallel. Although Spark is often deploy on the cloud, in this post we will work with Spark locally in R. I will be using a lot of functions from the sparklyr package, which has a dplyr backend to Spark. If you’re new to Spark and sparklyr, check out RStudio’s guide for getting started with Spark in R (<a href="https://spark.rstudio.com/">spark.rstudio.com/</a>).</p>
<p>Once you have Spark configured, import the data as a Spark dataframe.</p>
<pre class="r"><code>reddit_data &lt;- spark_read_csv(sc, &quot;parsed_json_to_csv_2013_05&quot;,
                              header = FALSE)</code></pre>
<p>Let’s see what this data looks like.</p>
<pre class="r"><code>head(reddit_data)</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 3]
##   V1                V2 V3
##   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;
## 1 shallah   1367452799 Documentaries
## 2 chowsers  1367452798 leagueoflegends
## 3 [deleted] 1367452796 nosleep
## 4 [deleted] 1367452796 woahdude
## 5 [deleted] 1367452796 starcraft
## 6 [deleted] 1367452796 aww</code></pre>
<p>Looks like we have lots of missing user names from deleted accounts. These rows will need to be deleted, and this could bias the results if missing accounts are unevenly distributed across subreddits (which they probably are). Let’s bracket this problem for now. Otherwise, this looks fine. Next, I make a few changes to the data. First, I change the column names. Next, I convert the time variable from utc time to the day of the year, which is easier to interpret. I also change the variable name to “day”. Finally, I remove submissions from users who have deleted their accounts from the data.</p>
<pre class="r"><code>reddit_data &lt;- reddit_data %&gt;%
    rename(author = V1, created_utc = V2, subreddit = V3) %&gt;%
    mutate(dateRestored = timestamp(created_utc + 18000)) %&gt;%
    mutate(day = dayofyear(dateRestored)) %&gt;%
    filter(author != &quot;[deleted]&quot;)</code></pre>
<pre class="r"><code>reddit_data %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 2377314</code></pre>
<p>After filtering out posts from deleted accounts, the number of Reddit submissions for March 2013 is 2,377,314. The next thing we need to do is filter Reddit submissions from users who only posted once. Based on how we’ve operationalized network ties, these users add nothing to our data. We also want to remove users who have posted too many times. Some Reddit accounts are bots that automatically posts to hundred of subreddits. Not only does including these accounts distort our understanding of how communities are organized on Reddit, but they cause our data to be exponentially larger for the reasons I explained above. Unfortunately, there’s no way to determine whether a user is a bot just based on the frequency with which they post. My arbitrary cutoff is 60. This cutoff is aggressive. In other words, it errs on the side of filtering humans who post a lot by human standards rather than erring on the side of keeping bots who post a little by bot standards.</p>
<pre class="r"><code>reddit_data &lt;- reddit_data %&gt;% group_by(author) %&gt;% mutate(count = count()) %&gt;%
    filter(count &lt; 60 &amp; count &gt; 1) %&gt;%
    ungroup()
reddit_data %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 1640677</code></pre>
<p>We’re down to 1,640,677 submissions, and we’re almost ready to convert our data to a network format. One last thing we need to is add a numeric id to each subreddit. This will prove useful for reasons I will explain later. To do this, I create a subreddit_key dataframe and merge this to the full dataframe. This will add an “id” column to the dataframe with a numeric id for each subreddit. I also pull the variables I want to use from the resulting data. These are “author”, “day”, “count”, “subreddit”, and “id.”</p>
<pre class="r"><code>subreddit_key &lt;- reddit_data %&gt;% distinct(subreddit) %&gt;% sdf_with_sequential_id()

reddit_data &lt;- left_join(reddit_data, subreddit_key, by = &quot;subreddit&quot;) %&gt;%
  select(author, day, count, subreddit, id)</code></pre>
<p>Let’s have a look at the data.</p>
<pre class="r"><code>head(reddit_data)</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 5]
##   author           day count subreddit             id
##   &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;
## 1 Bouda            141     4 100thworldproblems  2342
## 2 timeXalchemist   147     4 100thworldproblems  2342
## 3 babydall1267     144    18 123recipes          2477
## 4 babydall1267     144    18 123recipes          2477
## 5 babydall1267     144    18 123recipes          2477
## 6 babydall1267     144    18 123recipes          2477</code></pre>
<p>We now have 5 variables. The count variable shows the number of times a user has posted to Reddit. The id variable gives a numeric identifier to each subreddit. We are now ready to convert this data to network format. The first thing we need to do is take an inner_join of our data with itself by the “author” variable. For each user, the number of rows this returns will be the square of the number of submissions from that user. For example, the result of the code below shows that the user named “–will–” posted 7 times in the month of March.</p>
<pre class="r"><code>example_user &lt;- reddit_data %&gt;% filter(author == &quot;--will--&quot;)
example_user</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 5]
##   author     day count subreddit    id
##   &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;
## 1 --will--   125     7 pics       5043
## 2 --will--   132     7 pics       5043
## 3 --will--   142     7 pics       5043
## 4 --will--   122     7 videos     5042
## 5 --will--   137     7 AskReddit 15261
## 6 --will--   122     7 cars      15260
## 7 --will--   129     7 funny     28096</code></pre>
<p>If we take an inner_join and filter results from this user we will get 7^2 = 49 rows of data.</p>
<pre class="r"><code>example_user_network &lt;- inner_join(example_user, example_user %&gt;%
                                   rename(day2 = day, count2 = count,
                                          subreddit2 = subreddit, id2 = id),
                                   by = &quot;author&quot;)
example_user_network</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 9]
##    author     day count subreddit    id  day2 count2 subreddit2   id2
##    &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;
##  1 --will--   125     7 pics       5043   125      7 pics        5043
##  2 --will--   125     7 pics       5043   132      7 pics        5043
##  3 --will--   125     7 pics       5043   142      7 pics        5043
##  4 --will--   125     7 pics       5043   122      7 videos      5042
##  5 --will--   125     7 pics       5043   137      7 AskReddit  15261
##  6 --will--   125     7 pics       5043   122      7 cars       15260
##  7 --will--   125     7 pics       5043   129      7 funny      28096
##  8 --will--   132     7 pics       5043   125      7 pics        5043
##  9 --will--   132     7 pics       5043   132      7 pics        5043
## 10 --will--   132     7 pics       5043   142      7 pics        5043
## # … with more rows</code></pre>
<pre class="r"><code>example_user_network %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 49</code></pre>
<p>Our problem is that we only want combinations of unique subreddits to which “–will–” posted. –will– has posted to 5 unique subreddits (he posted to pics 3 times), so we want our data manipulations to yield 5 choose 2, or 10, rows of data. Let’s see how we can filter rows we don’t want to get the number of rows from 49 down to 21.</p>
<p>First, I filter rows in which subreddit pairs (labelled subreddit and subreddit2) are the same subreddit. We don’t want nodes in our network to be connected with themselves.</p>
<pre class="r"><code>example_user_network &lt;- example_user_network %&gt;% filter(subreddit != subreddit2)</code></pre>
<p>I also want to filter non-unique pairs of subreddits. Because –will– posted to pics three times, the inner_join will produce sets of rows for each of these submissions. We don’t want multiple submissions to the same subreddit to influence our results, so we remove these.</p>
<pre class="r"><code>example_user_network &lt;- example_user_network %&gt;%
                        group_by(author, subreddit, subreddit2) %&gt;%
                        filter(row_number(author) == 1)</code></pre>
<p>Let’s see the data for this user now looks.</p>
<pre class="r"><code>example_user_network</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 9]
## # Groups: author, subreddit, subreddit2
##    author     day count subreddit    id  day2 count2 subreddit2   id2
##    &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;
##  1 --will--   125     7 pics       5043   137      7 AskReddit  15261
##  2 --will--   125     7 pics       5043   122      7 cars       15260
##  3 --will--   122     7 videos     5042   122      7 cars       15260
##  4 --will--   137     7 AskReddit 15261   122      7 cars       15260
##  5 --will--   129     7 funny     28096   122      7 cars       15260
##  6 --will--   137     7 AskReddit 15261   125      7 pics        5043
##  7 --will--   137     7 AskReddit 15261   122      7 videos      5042
##  8 --will--   122     7 cars      15260   129      7 funny      28096
##  9 --will--   137     7 AskReddit 15261   129      7 funny      28096
## 10 --will--   122     7 videos     5042   129      7 funny      28096
## # … with more rows</code></pre>
<pre class="r"><code>example_user_network %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 20</code></pre>
<p>We’ve only managed to cut the data to 20 rows, not 10. What happened? The problem is with how we filtered duplicate rows above. Specifically, the grouping procedure used above considers links between two subreddits to be distinct if their own is different. For example, a row where subreddit is “pics” and subreddit2 is “AskReddit” is considered to belong to a distinct group from a row where subreddit is “AskReddit” and subreddit2 is “pics”. Unfortunately, I am not aware of a function in dplyr that filters based on multiple columns but regardless of the order of the items in those columns. How might we solve this problem?</p>
<p>This is where our id variables will prove useful. If we can mutate the two id variables into a variable that gives a unique identifier to each subreddit pair regardless of the order of subreddits, we can filter duplicates on the basis of this identifier. We need some kind of mathematical equation that takes two numbers and returns a unique number (i.e. a number that can only be produced from these two numbers) regardless of number order. One equation that does this is the Cantor Pairing Function (<a href="https://en.wikipedia.org/wiki/Pairing_function">wikipedia.org/wiki/Pairing_function</a>):</p>
<div class="figure">
<img src="/post/2019-07-22-mapping-the-underlying-social-structure-of-reddit_files/Screen%20Shot%202019-07-23%20at%208.21.48%20PM.png" style="width:75.0%" style="height:75.0%" />

</div>
<p>Let’s define a function in R that takes a dataframe and two id variables, runs the id variables through Cantor’s pairing function and appends this to the dataframe, filters duplicate cantor ids from the dataframe, and returns the result. We’ll call this the cantor_filter.</p>
<pre class="r"><code>cantor_filter &lt;- function(df, id, id2){
  df %&gt;% mutate(id_pair = .5*(id + id2)*(id + id2 + 1) + pmax(id, id2)) %&gt;% group_by(author, id_pair) %&gt;%
    filter(row_number(id_pair) == 1) %&gt;% return()
}</code></pre>
<p>Our hope is that when we pipe our Spark dataframe through cantor_filter we’ll get a dataframe with 10 rows.</p>
<pre class="r"><code>example_user_network %&gt;% cantor_filter()</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 10]
## # Groups: author, id_pair
##    author   day count subreddit    id  day2 count2 subreddit2   id2 id_pair
##    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;
##  1 --wil…   125     7 pics       5043   122      7 cars       15260  2.06e8
##  2 --wil…   137     7 AskReddit 15261   122      7 cars       15260  4.66e8
##  3 --wil…   137     7 AskReddit 15261   122      7 videos      5042  2.06e8
##  4 --wil…   122     7 videos     5042   122      7 cars       15260  2.06e8
##  5 --wil…   129     7 funny     28096   122      7 cars       15260  9.40e8
##  6 --wil…   125     7 pics       5043   137      7 AskReddit  15261  2.06e8
##  7 --wil…   137     7 AskReddit 15261   129      7 funny      28096  9.40e8
##  8 --wil…   125     7 pics       5043   122      7 videos      5042  5.09e7
##  9 --wil…   122     7 videos     5042   129      7 funny      28096  5.49e8
## 10 --wil…   129     7 funny     28096   125      7 pics        5043  5.49e8</code></pre>
<p>Great, it worked. Pay attention to the subreddit and subreddit2 columns and notice that they only contain unique pairs of subreddits. These are connections between subreddits for which this user is responsible. Next, I apply the steps detailed above to the entire dataframe, not just to –will–’s submissions.</p>
<pre class="r"><code>reddit_network_data &lt;- inner_join(reddit_data, reddit_data %&gt;%
                        rename(day2 = day, count2 = count,
                        subreddit2 = subreddit, id2 = id),
                        by = &quot;author&quot;) %&gt;%
           filter(subreddit != subreddit2) %&gt;%
           group_by(author, subreddit, subreddit2) %&gt;%
           filter(row_number(author) == 1) %&gt;%
           cantor_filter() %&gt;%
           select(author, subreddit, subreddit2, id, id2, day, day2, id_pair) %&gt;%
           ungroup %&gt;% arrange(author)</code></pre>
<p>Let’s take a look at the new data and see how many rows it has.</p>
<pre class="r"><code>reddit_network_data</code></pre>
<pre><code>## # Source:     spark&lt;?&gt; [?? x 8]
## # Ordered by: author
##    author     subreddit     subreddit2        id   id2   day  day2  id_pair
##    &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
##  1 --5Dhere   depression    awakened        7644 29936   135   135   7.06e8
##  2 --Adam--   AskReddit     techsupport    15261 28113   135   142   9.41e8
##  3 --Caius--  summonerscho… leagueoflegen…    79     3   124   142   3.48e3
##  4 --Gianni-- gaming        movies         10158 20348   131   124   4.65e8
##  5 --Gianni-- gaming        AskReddit      10158 15261   131   125   3.23e8
##  6 --Gianni-- gaming        pics           10158  5043   131   126   1.16e8
##  7 --Gianni-- AskReddit     videos         15261  5042   125   138   2.06e8
##  8 --Gianni-- pics          videos          5043  5042   126   138   5.09e7
##  9 --Gianni-- movies        pics           20348  5043   124   126   3.22e8
## 10 --Gianni-- pics          AskReddit       5043 15261   126   125   2.06e8
## # … with more rows</code></pre>
<pre class="r"><code>reddit_network_data %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 1685921</code></pre>
<p>Surprisingly, the network data is only 45,000 more rows than the original dataset. This is likely due to the fact that we filtered users who posted more than 60 times, so the unique combinations of subreddits to which users have posted is not that much larger than the number of unique submissions per user. This result is also a sign that many users post repeatly to the same subreddit rather than posted to multiple subreddits.</p>
<p>I should note that this does not obviate the need to work within Spark. Before filtering duplicates, our dataframe was over 16 million rows. Thus, these data manipulations run a lot faster when we manipulate the data as a Spark dataframe rather than as a regular dataframe.</p>
<pre class="r"><code># Number of rows in the data before filtering duplicate rows
inner_join(reddit_data, reddit_data %&gt;%
                        rename(day2 = day, count2 = count,
                        subreddit2 = subreddit, id2 = id),
                        by = &quot;author&quot;) %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 16286137</code></pre>
<p>We now have a ~1.6 row Spark dataframe where each row represents a link between two subreddits through a distinct user. Many pairs of subreddits are connected by multiple users. We can think of subreddit pairs connected through more users as being more connected than subreddit pairs connected by fewer users. With this rationale, I create a “weight” variable that tallies the number of users connecting each subreddit pair and then filter the dataframe to unique pairs.</p>
<pre class="r"><code>reddit_network_data &lt;- reddit_network_data %&gt;% group_by(id_pair) %&gt;%
  mutate(weight = n()) %&gt;% filter(row_number(id_pair) == 1) %&gt;%
  ungroup</code></pre>
<p>Let’s have a look at the data and see how many rows it has.</p>
<pre class="r"><code>reddit_network_data</code></pre>
<pre><code>## # Source:     spark&lt;?&gt; [?? x 9]
## # Ordered by: author
##    author     subreddit   subreddit2    id   id2   day  day2 id_pair weight
##    &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1 h3rbivore  psytrance   DnB            8     2   142   142      63      1
##  2 StRefuge   findareddit AlienBlue     23     5   133   134     429      1
##  3 DylanTho   blackops2   DnB           28     2   136   138     493      2
##  4 TwoHardCo… bikewrench  DnB           30     2   137   135     558      1
##  5 Playbook4… blackops2   AlienBlue     28     5   121   137     589      2
##  6 A_Jewish_… atheism     blackops2      6    28   139   149     623     14
##  7 SirMechan… Terraria    circlejerk    37     7   150   143    1027      2
##  8 Jillatha   doctorwho   facebookw…    36     9   131   147    1071      2
##  9 MeSire     Ebay        circlejerk    39     7   132   132    1120      3
## 10 Bluesfan6… SquaredCir… keto          29    18   126   134    1157      2
## # … with more rows</code></pre>
<pre class="r"><code>reddit_network_data %&gt;% sdf_nrow</code></pre>
<pre><code>## [1] 744939</code></pre>
<p>We’re down to ~750,000 rows. The weight column shows that many of the subreddit pairs in our data are only connected by 1 or 2 users. We can substantially reduce the size of the data without losing the subreddit pairs we’re interested in by filtering these rows. I decided to remove subreddit pairs that are connected by 3 or fewer users. I also select the id variables and the weight variable and bring the data into R. The network analytic tools I use next require working on a regular dataframe and our data is now small enough that we can do this without any problems. Because we’re moving into the R workspace, I save this as a new dataframe called reddit_edgelist.</p>
<pre class="r"><code> reddit_edgelist &lt;- reddit_network_data %&gt;% filter(weight &gt; 3) %&gt;%
  select(id, id2, weight) %&gt;% arrange(id) %&gt;%
  # Bringing the data into the R workspace
  dplyr::collect()</code></pre>
<p>Our R dataframe consists of three columns: two id columns that provide information on connections between nodes and a weight column that tells us the strength of each connection. One nice thing to have would be some kind of metric for the relative importance of each subreddit. A simple way to get that would be to count how many times each subreddit appears in the data. I compute this for each subreddit by adding the weight value in the rows where that subreddit appears (see code annotation for deeper explanation of this). I create a dataframe called subreddit_imp_key that lists subreddit ids by subreddit importance.</p>
<pre class="r"><code>subreddit_imp_key &lt;- full_join(reddit_edgelist %&gt;% group_by(id) %&gt;%
                                 summarise(count = sum(weight)),
            reddit_edgelist %&gt;% group_by(id2) %&gt;%
              summarise(count2 = sum(weight)),
            by = c(&quot;id&quot; = &quot;id2&quot;)) %&gt;%
            mutate(count = ifelse(is.na(count), 0, count)) %&gt;%
            mutate(count2 = ifelse(is.na(count2), 0, count2)) %&gt;%
            mutate(id = id, imp = count + count2) %&gt;% select(id, imp) </code></pre>
<p>To check it this worked I merging the result with the subreddit_key (which has subreddit names by their id) and arrange by subreddit importance.</p>
<pre class="r"><code>left_join(subreddit_imp_key, subreddit_key %&gt;% dplyr::collect(), by = &quot;id&quot;) %&gt;%
  arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 5,561 x 3
##       id    imp subreddit
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;
##  1 28096 107894 funny
##  2 15261 101239 AskReddit
##  3 20340  81208 AdviceAnimals
##  4  5043  73119 pics
##  5 10158  51314 gaming
##  6  5042  47795 videos
##  7 17856  47378 aww
##  8  2526  37311 WTF
##  9 22888  31702 Music
## 10  5055  26666 todayilearned
## # … with 5,551 more rows</code></pre>
<p>These appear be some of the most popular subreddits. They’re mostly about memes and gaming, which are two things that people commonly associate with Reddit.</p>
<p>Next, I reweight the edge weights in reddit_edgelist by subreddit importance. The reason I do this is that the number of users connecting subreddits is partially a function of subreddit popularity. I want edge weights to instead track the overlap in subreddit communities. By reweighted by subreddit importance, I control for the influence of this potentially confounding variable.</p>
<pre class="r"><code>reddit_edgelist &lt;- left_join(reddit_edgelist, subreddit_imp_key,
                             by = c(&quot;id&quot; = &quot;id&quot;)) %&gt;%
                  left_join(., subreddit_imp_key %&gt;% rename(imp2 = imp),
                            by = c(&quot;id2&quot; = &quot;id&quot;)) %&gt;%
  mutate(imp_fin = (imp + imp2)/2) %&gt;% mutate(weight = weight/imp_fin) %&gt;%
  select(id, id2, weight)

reddit_edgelist</code></pre>
<pre><code>## # A tibble: 56,257 x 3
##       id   id2   weight
##    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1     1 12735 0.0141
##  2     1 10158 0.000311
##  3     1  2601 0.00602
##  4     1 17856 0.000505
##  5     1 22900 0.000488
##  6     1 25542 0.0185
##  7     1 15260 0.00638
##  8     1 20340 0.000320
##  9     2  2770 0.0165
## 10     2 15261 0.000295
## # … with 56,247 more rows</code></pre>
<p>We now have our final edgelist. There are about 56,000 thousand rows in the data, though most edges have very small weights. Next, I use the igraph package to turn this dataframe into a graph object. Graph objects can be analyzed using igraph’s clustering algorithms. Let’s have a look at what this graph object looks like.</p>
<pre class="r"><code>reddit_graph &lt;- graph_from_data_frame(reddit_edgelist, directed = FALSE)
reddit_graph</code></pre>
<pre><code>## IGRAPH e14409c UNW- 5561 56257 --
## + attr: name (v/c), weight (e/n)
## + edges from e14409c (vertex names):
##  [1] 1--12735 1--10158 1--2601  1--17856 1--22900 1--25542 1--15260
##  [8] 1--20340 2--2770  2--15261 2--18156 2--20378 2--41    2--22888
## [15] 2--28115 2--10172 2--5043  2--28408 2--2553  2--2836  2--28096
## [22] 2--23217 2--17896 2--67    2--23127 2--2530  2--2738  2--7610
## [29] 2--20544 2--25566 2--3     2--7     2--7603  2--12931 2--17860
## [36] 2--6     2--2526  2--5055  2--18253 2--22996 2--25545 2--28189
## [43] 2--10394 2--18234 2--23062 2--25573 3--264   3--2599  3--5196
## [50] 3--7585  3--10166 3--10215 3--12959 3--15293 3--20377 3--20427
## + ... omitted several edges</code></pre>
<p>Here we have a list of all of the edges from the dataframe, represented as subreddit ids connected by “–”. We can now use a clustering algorithm to analyze the community structure that underlies this subreddit network. The clustering algorithm I choose to use here is the Louvain algorithm. I opt to use this algorithm because it is fast and easy to interpret. The Louvain algorithm takes a network and groups its nodes into different communities in a way that maximizes the modularity of the resulting network. Modularity is a measure of network structure. Networks with high modularity consist of nodes that are grouped in such a way that nodes are densely connected within node groups and sparsely connected between node groups. By maximizing modularity, the Louvain algorithm finds a grouping of nodes that maximizes the number of within-group ties and minimizes the number of between-group ties.</p>
<p>Let’s apply the algorithm and see if the groupings it produces make sense. Make sure to incorporate weights by setting the weights parameter to the weight column from reddit_edgelist. I’m going to store the results of the algorithm in a tibble. See code annotations to understand what’s going on here.</p>
<pre class="r"><code>reddit_communities &lt;- cluster_louvain(reddit_graph, weights = reddit_edgelist$weight)

subreddit_by_comm &lt;- tibble(
  # Using map from purrr to extract subreddit ids from reddit_communities
  id = map(reddit_communities[], as.numeric) %&gt;% unlist,
  # Creating a community ids column and using rep function with map to populate
  # column with community ids created by
  # Louvain alg
  comm = rep(reddit_communities[] %&gt;%
               names, map(reddit_communities[], length) %&gt;% unlist) %&gt;%
               as.numeric) %&gt;%
  # Adding subreddit names
  left_join(., subreddit_key %&gt;% dplyr::collect(), by = &quot;id&quot;) %&gt;%
  # Keeping subreddit name, subreddit id, community id
  select(subreddit, id, comm) %&gt;%
  # Adding subreddit  importance
  left_join(., subreddit_imp_key, by = &quot;id&quot;)

subreddit_by_comm</code></pre>
<pre><code>## # A tibble: 5,561 x 4
##    subreddit          id  comm   imp
##    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 resumes          2636     1   106
##  2 Resume           6001     1    14
##  3 Norway             93     2    36
##  4 DANMAG           2778     2    36
##  5 Nordiccountries  2974     2     8
##  6 Denmark          5211     2   161
##  7 Swanada          6177     2     4
##  8 SWARJE           7676     2    92
##  9 DanishEnts       8042     2     8
## 10 Aarhus          11176     2     4
## # … with 5,551 more rows</code></pre>
<p>From these 10 rows at least, the groups appear to make sense. Community 1 consists of two subreddits, resumes and Resume (a resume community), and community 2 consists of several subreddits on Denmark and Norway and some other nearby places (a Nordic community?). Still, these communities are pretty small. Let’s calculate community importance by summing the subreddit importance scores of the subreddits in each community.</p>
<pre class="r"><code>subreddit_by_comm &lt;- subreddit_by_comm %&gt;% group_by(comm) %&gt;% mutate(comm_imp = sum(imp)) %&gt;% ungroup </code></pre>
<p>Now let’s store a vector of id’s in order of community importance.</p>
<pre class="r"><code>comm_ids &lt;- subreddit_by_comm %&gt;% group_by(comm) %&gt;% slice(1) %&gt;% arrange(desc(comm_imp)) %&gt;% .[[&quot;comm&quot;]]</code></pre>
<p>Let’s have a look at the most important community arranged by importance of subreddits within that community.</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[1]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 2,209 x 5
##    subreddit        id  comm    imp comm_imp
##    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1 funny         28096   154 107894  1364159
##  2 AskReddit     15261   154 101239  1364159
##  3 AdviceAnimals 20340   154  81208  1364159
##  4 pics           5043   154  73119  1364159
##  5 gaming        10158   154  51314  1364159
##  6 videos         5042   154  47795  1364159
##  7 aww           17856   154  47378  1364159
##  8 WTF            2526   154  37311  1364159
##  9 Music         22888   154  31702  1364159
## 10 todayilearned  5055   154  26666  1364159
## # … with 2,199 more rows</code></pre>
<p>These are literally the 10 most popular subreddits on Reddit. It’s the same 10 subreddits that I showed earlier. Although the subreddits on this list are somewhat similar - about half of them are for distributing memes - we see a lot of subreddits that don’t have any obvious relevance to each other (Music and AdviceAnimals, for instance). Even more importantly, notice that this tibble consists of ~2,200 rows. That is almost half of our dataset. What went wrong here? One possibility is that this community represents a residual category, subreddits that traverse multiple communities. While the Louvain algorithm forces nodes into single groups, there’s no reason to assume that nodes belong to one particular community. It’s likely that some of these subreddits have users that span many communities on Reddit. In this post, I have opted to go with the Louvain algorithm for practical reasons. Stanford’s Network Analysis Project has powerful tools for implementing fuzzy clustering for large networks. Unfortunately, these tools have yet to be implemented in R, though they are available in Python (<a href="http://snap.stanford.edu/snappy/index.html">snap.stanford.edu/snappy/index.html</a>). I may attempt to apply these tools to the problem of Reddit community detection in a subsequent post.</p>
<p>A second more sanguine possibility is that these subreddits do comprise a community - mainstream Reddit. It’s possible that Reddit consists of large core of subreddits devoted to memes, gaming, and advice, and a much smaller periphery dedicated to the discussion of specific topics (like resumes). My suspicion is that what’s going on here is some combination of these two possibilities. In other words, this group of subreddits is a mix of core subreddits and other subreddits than don’t fit into any of the other communities.</p>
<p>Let’s have a lot at the second biggest community that the algorithm identified.</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[2]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 263 x 5
##    subreddit            id  comm   imp comm_imp
##    &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 DotA2              2525    76  3408    52550
##  2 tf2                7582    76  2853    52550
##  3 SteamGameSwap     15275    76  2804    52550
##  4 starcraft         22893    76  2286    52550
##  5 tf2trade          22921    76  2171    52550
##  6 Dota2Trade        25524    76  1619    52550
##  7 GiftofGames       22912    76  1499    52550
##  8 SteamTradingCards 17859    76  1245    52550
##  9 Steam              2582    76  1235    52550
## 10 vinyl             22939    76  1152    52550
## # … with 253 more rows</code></pre>
<p>This appears to be some kind of trading/online gaming community. Let’s look at the third biggest community.</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[3]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 208 x 5
##    subreddit              id  comm   imp comm_imp
##    &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 electronicmusic     20378   161  2610    24570
##  2 dubstep             25573   161  1210    24570
##  3 WeAreTheMusicMakers 17938   161  1071    24570
##  4 futurebeats          5053   161   751    24570
##  5 trap                18062   161   704    24570
##  6 edmproduction          67   161   694    24570
##  7 electrohouse           43   161   647    24570
##  8 EDM                 20544   161   641    24570
##  9 punk                20413   161   604    24570
## 10 ThisIsOurMusic       5156   161   565    24570
## # … with 198 more rows</code></pre>
<p>Music (mostly electronic)! And the fourth.</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[4]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 171 x 5
##    subreddit          id  comm   imp comm_imp
##    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 hockey           5066    85  3941    20496
##  2 fantasybaseball  5194    85   767    20496
##  3 nhl              7605    85   725    20496
##  4 Austin             97    85   599    20496
##  5 DetroitRedWings 25670    85   542    20496
##  6 sanfrancisco     2744    85   445    20496
##  7 houston         10184    85   435    20496
##  8 leafs            5148    85   406    20496
##  9 BostonBruins    15317    85   382    20496
## 10 mlb              7845    85   365    20496
## # … with 161 more rows</code></pre>
<p>Sports (and cities?)! And the fifth.</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[5]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 123 x 5
##    subreddit          id  comm   imp comm_imp
##    &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 cars            15260    87  3300    18887
##  2 motorcycles     12721    87  2776    18887
##  3 Autos            2601    87  1180    18887
##  4 sysadmin        20422    87  1011    18887
##  5 carporn         12735    87   699    18887
##  6 formula1         7749    87   656    18887
##  7 Jeep            25577    87   601    18887
##  8 subaru          22925    87   573    18887
##  9 Cartalk           120    87   499    18887
## 10 techsupportgore 10323    87   438    18887
## # … with 113 more rows</code></pre>
<p>Cars! The sixth?</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[6]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 138 x 5
##    subreddit       id  comm   imp comm_imp
##    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 web_design   10189    56  1414    17926
##  2 Entrepreneur  7698    56  1172    17926
##  3 programming  25544    56  1136    17926
##  4 webdev       10255    56  1066    17926
##  5 Design       22920    56  1059    17926
##  6 windowsphone 25535    56   737    17926
##  7 SEO          20386    56   664    17926
##  8 forhire         68    56   596    17926
##  9 startups     12757    56   569    17926
## 10 socialmedia  25585    56   537    17926
## # … with 128 more rows</code></pre>
<p>Web design! The seventh?</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[7]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 73 x 5
##    subreddit        id  comm   imp comm_imp
##    &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 itookapicture 25556   109  3307    15074
##  2 EarthPorn     22899   109  1560    15074
##  3 AbandonedPorn 10252   109   807    15074
##  4 HistoryPorn   10253   109   777    15074
##  5 photocritique  5190   109   725    15074
##  6 CityPorn       7646   109   675    15074
##  7 MapPorn        7593   109   650    15074
##  8 AnimalPorn    20428   109   549    15074
##  9 SkyPorn       12774   109   479    15074
## 10 Astronomy     12820   109   465    15074
## # … with 63 more rows</code></pre>
<p>Cool photos! The eighth?</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[8]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 82 x 5
##    subreddit      id  comm   imp comm_imp
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 wow         15288   128  2193    12978
##  2 darksouls   25558   128  1471    12978
##  3 Diablo      10186   128  1359    12978
##  4 Neverwinter 25549   128  1253    12978
##  5 Guildwars2   7598   128   967    12978
##  6 runescape   20388   128   784    12978
##  7 diablo3     20342   128   465    12978
##  8 2007scape   17871   128   372    12978
##  9 swtor        2630   128   318    12978
## 10 Smite       15342   128   287    12978
## # … with 72 more rows</code></pre>
<p>More gaming stuff (I think)! The ninth?</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[9]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 94 x 5
##    subreddit       id  comm   imp comm_imp
##    &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 blackops2       28   153  1563    11856
##  2 battlefield3 20385   153  1470    11856
##  3 dayz         20439   153   871    11856
##  4 Eve          10214   153   733    11856
##  5 Planetside   10241   153   714    11856
##  6 aviation     12886   153   675    11856
##  7 airsoft      17928   153   557    11856
##  8 WorldofTanks 22940   153   433    11856
##  9 Warframe      5143   153   362    11856
## 10 CallOfDuty   20344   153   320    11856
## # … with 84 more rows</code></pre>
<p>War gaming/actual warfare! The tenth?</p>
<pre class="r"><code>subreddit_by_comm %&gt;% filter(comm == comm_ids[10]) %&gt;% arrange(desc(imp))</code></pre>
<pre><code>## # A tibble: 102 x 5
##    subreddit      id  comm   imp comm_imp
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 soccer       7610   174  4417    11398
##  2 Seattle     28183   174   843    11398
##  3 Fifa13      12724   174   508    11398
##  4 Portland     7736   174   496    11398
##  5 MLS         10205   174   386    11398
##  6 Gunners      7609   174   371    11398
##  7 reddevils    7661   174   348    11398
##  8 chelseafc   25650   174   340    11398
##  9 football    20529   174   329    11398
## 10 LiverpoolFC   119   174   218    11398
## # … with 92 more rows</code></pre>
<p>Soccer (and cities in the northwest?)! Okay, you get the idea. In sum, the algorithm does an alright job of identifying smaller communities on Reddit. Mainstream Reddit/subreddits that elude categorization have been placed in a large residual group.</p>
<p>We have somewhat successfully taken a month worth of Reddit submissions and identified subreddit communities from them. In the next section, I will use some of R’s network graphing tools to visualize these communities as a network.</p>
</div>
<div id="visualizations" class="section level1">
<h1>Visualizations</h1>
<p>The first thing I want to do is return to the subreddit edgelist (reddit_edgelist) and add community id variables corresponding to each subreddit. I then filter the dataframe to only include unique edges. I also add a variable called weight_fin which is the mean of the subreddit edge weights between each community.</p>
<pre class="r"><code>community_edgelist &lt;- left_join(reddit_edgelist, subreddit_by_comm %&gt;% select(id, comm), by = &quot;id&quot;) %&gt;%
  left_join(., subreddit_by_comm %&gt;% select(id, comm) %&gt;% rename(comm2 = comm), by = c(&quot;id2&quot;= &quot;id&quot;)) %&gt;%
  select(comm, comm2, weight) %&gt;%
  mutate(id_pair = .5*(comm + comm2)*(comm + comm2 + 1) + pmax(comm,comm2)) %&gt;% group_by(id_pair) %&gt;%
  mutate(weight_fin = mean(weight)) %&gt;% slice(1) %&gt;% ungroup %&gt;% select(comm, comm2, weight_fin) %&gt;%
  filter(comm != comm2)</code></pre>
<p>We now have a community-level edgelist, consisting of community ids for the community nodes connected by each edge. Many of the edges in this dataframe are connecting nodes to themselves (i.e. comm = comm2). We want to remove these. Let’s look at the data.</p>
<pre class="r"><code>community_edgelist &lt;- community_edgelist %&gt;% filter(comm != comm2) %&gt;%
  arrange(desc(weight_fin))
community_edgelist</code></pre>
<pre><code>## # A tibble: 532 x 3
##     comm comm2 weight_fin
##    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;
##  1   133    71     0.0388
##  2   171   196     0.0327
##  3    48   161     0.0299
##  4   118   156     0.0281
##  5    83   109     0.0273
##  6   155   143     0.0257
##  7    85    83     0.0254
##  8   163   105     0.0253
##  9   171   133     0.0250
## 10   100    56     0.0240
## # … with 522 more rows</code></pre>
<p>One potential problem is that largest weights (strongest ties) all seem to be between the biggest communities.</p>
<p>I’m now ready to visualize this network of subreddit communities. The first thing I need to do is modify the edge weight variable to discriminate between communities that are more and less connected. I choose an arbitrary cutoff point (.007) and set all weights below this cutoff to 0. Although doing this creates a risk of imposing structure on the network where there is none, this cutoff will also help highlight significant ties between communities.</p>
<pre class="r"><code>community_edgelist_ab &lt;- community_edgelist %&gt;%
  mutate(weight =  ifelse(weight_fin &gt; .007, weight_fin, 0)) %&gt;%
  filter(weight!=0) %&gt;% mutate(weight = abs(log(weight)))</code></pre>
<p>The visualization tools that I use here come from the visnetwork package. For an excellent set of tutorials on network visualizations in R, check out the tutorials section of Professor Katherine Ognyanova’s website (<a href="https://kateto.net/tutorials/">kateto.net/tutorials/</a>). Much of what I know about network visualization in R I learned from the “Static and dynamic network visualization in R” tutorial.</p>
<p>Visnetwork’s main function, visNetwork, requires two arguments, one for nodes data and one for edges data. These dataframes need to have particular column names for visnetwork to be able to make sense of them. Let’s start with the edges data. The column names for the nodes corresponding to edges in the edgelist need to be “from” and “to”, and the column name for edge weights needs to be weight. I make these adjustments.</p>
<pre class="r"><code>community_edgelist_mod &lt;- community_edgelist_ab %&gt;%
  rename(from = comm, to = comm2) %&gt;% select(from, to, weight) </code></pre>
<p>Also, visnetwork’s default edges are curved. I prefer straight edges. To ensure edges are straight, add a smooth column and set it to <code>FALSE</code>.</p>
<pre class="r"><code>community_edgelist_mod$smooth &lt;- F</code></pre>
<p>I’m not ready to set up the nodes data. First, I extract all nodes from the community edgelist.</p>
<pre class="r"><code>community_nodes &lt;- c(community_edgelist_mod %&gt;% .[[&quot;from&quot;]], community_edgelist_mod %&gt;% .[[&quot;to&quot;]]) %&gt;% unique</code></pre>
<p>Visnetwork has this really cool feature that lets you view node labels by hovering over them with your mouse cursor. This is made possible by the fact that the package renders visualizations in javascript. I’m going to label each community with the names of the 4 most popular subreddits in that community.</p>
<pre class="r"><code>comm_by_label &lt;- subreddit_by_comm %&gt;% arrange(comm, desc(imp)) %&gt;% group_by(comm) %&gt;% slice(1:4) %&gt;%
  summarise(title = paste(subreddit, collapse = &quot; &quot;))</code></pre>
<p>Next, I put node ids and community labels in a tibble. Note that the label column in this tibble has to be called “title”.</p>
<pre class="r"><code>community_nodes_fin &lt;- tibble(comm = community_nodes) %&gt;% left_join(., comm_by_label, by = &quot;comm&quot;)</code></pre>
<p>I want the nodes of my network to vary in size based on the size of each community. To do this, I create a community importance key. I’ve already calculated community importance above. I simply extract this score for each community from the subreddit_by_comm dataframe. I then merge the importance scores with the nodes data. I rename the community importance variable “size”, which is the node size column name that visnetwork recognizes.</p>
<pre class="r"><code>comm_imp_key &lt;- subreddit_by_comm %&gt;% group_by(comm) %&gt;% slice(1) %&gt;%
  arrange(desc(comm_imp)) %&gt;% select(comm, comm_imp)

community_nodes_fin &lt;- inner_join(community_nodes_fin, comm_imp_key, by = &quot;comm&quot;) %&gt;%
  rename(size = comm_imp) </code></pre>
<p>One issue is that my “mainstream Reddit/residual subreddits” community is that it is so much bigger than the other communities in the data that the network visualization will be overtaken by it if I don’t adjust the size variable. I remedy this by raising community size to the “.3”th power (close to the cube root).</p>
<pre class="r"><code>community_nodes_fin &lt;- community_nodes_fin %&gt;% mutate(size = size^.3)</code></pre>
<p>I can now enter the nodes and edges data into the visNetwork function. One last change to the nodes data - I had forgotten to name the node id column “id”. This is necessary for visnetwork to work. I make a few adjustments to the default parameters. One newish feature of visnetwork is that it lets you use layouts from the igraph package. I use visIgraphLayout to set the position of the nodes according to the Fruchterman-Reingold Layout Algorithm (layout_with_fr). I also adjust edge widths and highlightNearest to <code>TRUE</code>. This lets you highlight a node and the nodes it is connected to by clicking on it. Without further ado, let’s look at the network.</p>
<pre class="r"><code>visNetwork(community_nodes_fin %&gt;% rename(id = comm), community_edgelist_mod) %&gt;%
  visIgraphLayout(layout = &quot;layout_with_fr&quot;) %&gt;%
  visEdges(width = 3.5) %&gt;% visOptions(., highlightNearest = TRUE)</code></pre>
<iframe src="post/mapping-the-underlying-social-structure-of-reddit/net_2013.html"></iframe>

		<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

	</div>

	<div class="pagination">
		<a href="/post/building-a-recommendation-system-with-beer-data/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			<span>
			&copy; <time datetime="2019-07-25 11:00:39.899506 -0400 EDT m=&#43;7.647509318">2019</time> Andrew Carr. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
